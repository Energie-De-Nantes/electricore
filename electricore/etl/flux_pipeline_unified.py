"""
Pipeline ETL unifiÃ© pour les flux Enedis.
Combine la logique d'extraction d'electriflux avec les capacitÃ©s DLT.
"""

import dlt
import yaml
from pathlib import Path
from typing import Iterator
from dlt.sources.filesystem import FileItemDict, filesystem
from xml_to_dict import xml_to_dict

# Configuration
ETL_DIR = Path(__file__).parent
CONFIG_FILE = ETL_DIR / "simple_flux.yaml"
BUCKET_URL = "file:///home/virgile/data/flux_enedis/"

# Charger la configuration YAML
with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
    FLUX_CONFIG = yaml.safe_load(f)

print(f"ğŸ“‹ Configuration chargÃ©e pour {len(FLUX_CONFIG)} types de flux: {list(FLUX_CONFIG.keys())}")


@dlt.transformer
def parse_flux_xml(items: Iterator[FileItemDict], flux_type: str) -> Iterator[dict]:
    """
    Transformer gÃ©nÃ©rique qui utilise la config YAML pour parser chaque type de flux.
    """
    config = FLUX_CONFIG[flux_type]
    count = 0
    
    print(f"ğŸ”„ Traitement flux {flux_type}...")
    
    for file_obj in items:
        try:
            with file_obj.open() as file:
                # Utiliser xml_to_dict avec la config du flux
                for record in xml_to_dict(
                    file,
                    row_level=config['row_level'],
                    metadata_fields=config.get('metadata_fields', {}),
                    data_fields=config.get('data_fields', {}),
                    nested_fields=config.get('nested_fields', [])
                ):
                    count += 1
                    
                    # Ajouter mÃ©tadonnÃ©es DLT pour le suivi
                    record['_flux_type'] = flux_type
                    record['_file_name'] = file_obj['file_name']
                    record['_file_path'] = file_obj['relative_path']
                    record['_file_modified'] = file_obj['modification_date']
                    
                    # Log de progression tous les 100 enregistrements
                    if count % 100 == 0:
                        print(f"   â³ {flux_type}: {count} enregistrements traitÃ©s")
                    
                    yield record
                    
        except Exception as e:
            print(f"âŒ Erreur lors du traitement de {file_obj['file_path']}: {e}")
            continue
    
    print(f"âœ… {flux_type}: {count} enregistrements au total")


@dlt.source
def flux_enedis():
    """
    Source DLT dynamique basÃ©e sur la configuration YAML.
    CrÃ©e automatiquement une ressource par type de flux configurÃ©.
    """
    resources = []
    
    for flux_type, config in FLUX_CONFIG.items():
        print(f"ğŸ—ï¸  CrÃ©ation de la ressource pour {flux_type}")
        
        # Construire le pattern de fichiers - limiter Ã  1 fichier pour test initial
        if 'file_regex' in config:
            # Utiliser le regex spÃ©cifique s'il existe
            file_pattern = f"**/{config['file_regex']}"
        else:
            # Pattern par dÃ©faut basÃ© sur le nom du flux
            file_pattern = f"{flux_type}/**/*.xml"
        
        print(f"   ğŸ” Pattern de fichiers: {file_pattern}")
        
        # CrÃ©er le pipeline filesystem | transformer pour ce flux
        flux_pipeline = (
            filesystem(bucket_url=BUCKET_URL, file_glob=file_pattern) 
            | parse_flux_xml(flux_type=flux_type)
        )
        
        # Le nom de la ressource est dÃ©fini via with_name()
        flux_pipeline = flux_pipeline.with_name(f"flux_{flux_type.lower()}")
        
        # DÃ©terminer la clÃ© primaire selon le type de flux
        if flux_type in ['R15', 'R151', 'R15_ACC']:
            # Flux de relevÃ©s : PDL + Date
            primary_key = ["pdl", "Date_Releve"]
        elif flux_type in ['F12', 'F15']:
            # Flux de facturation : PDL + Facture + Element valorisÃ©
            primary_key = ["pdl", "Num_Facture", "Id_EV"] 
        else:  # C15
            # Flux contractuels : PDL + Date Ã©vÃ©nement
            primary_key = ["pdl", "Date_Evenement"]
        
        # Appliquer les hints DLT
        flux_pipeline.apply_hints(
            primary_key=primary_key,
            write_disposition="merge",  # Merge pour gÃ©rer les doublons
            incremental=dlt.sources.incremental("_file_modified")  # IncrÃ©mental sur modification
        )
        
        resources.append(flux_pipeline)
    
    return resources


def run_pipeline():
    """
    ExÃ©cute le pipeline complet et affiche les rÃ©sultats.
    """
    print("ğŸš€ DÃ©marrage du pipeline DLT unifiÃ©...")
    
    # CrÃ©er le pipeline DLT
    pipeline = dlt.pipeline(
        pipeline_name="flux_enedis_unified",
        destination="duckdb",
        dataset_name="flux_enedis"
    )
    
    # ExÃ©cuter le chargement
    try:
        load_info = pipeline.run(flux_enedis())
        print("âœ… Pipeline exÃ©cutÃ© avec succÃ¨s !")
        print(f"ğŸ“Š RÃ©sultats du chargement:")
        print(load_info)
        
    except Exception as e:
        print(f"âŒ Erreur lors de l'exÃ©cution du pipeline: {e}")
        return False
    
    # VÃ©rifier et afficher les rÃ©sultats dans DuckDB
    verify_results()
    return True


def verify_results():
    """
    VÃ©rifie et affiche le nombre d'enregistrements chargÃ©s par table.
    """
    print("\nğŸ“ˆ VÃ©rification des rÃ©sultats dans DuckDB...")
    
    try:
        import duckdb
        conn = duckdb.connect('flux_enedis_unified.duckdb')
        
        # Lister les tables crÃ©Ã©es
        tables = conn.execute("SELECT table_name FROM information_schema.tables WHERE table_schema = 'flux_enedis'").fetchall()
        
        if not tables:
            print("âŒ Aucune table trouvÃ©e dans le schÃ©ma flux_enedis")
            return
        
        print(f"ğŸ“‹ Tables crÃ©Ã©es: {[t[0] for t in tables]}")
        
        # Compter les enregistrements par table
        total_records = 0
        for table_name, in tables:
            try:
                count = conn.execute(f"SELECT COUNT(*) FROM flux_enedis.{table_name}").fetchone()[0]
                print(f"   ğŸ“Š {table_name}: {count:,} enregistrements")
                total_records += count
            except Exception as e:
                print(f"   âŒ Erreur pour la table {table_name}: {e}")
        
        print(f"\nğŸ¯ Total: {total_records:,} enregistrements chargÃ©s")
        
        # Afficher un Ã©chantillon de donnÃ©es
        if total_records > 0:
            print("\nğŸ” Ã‰chantillon de donnÃ©es (premiÃ¨re table):")
            first_table = tables[0][0]
            sample = conn.execute(f"SELECT * FROM flux_enedis.{first_table} LIMIT 3").fetchall()
            columns = [desc[0] for desc in conn.description]
            
            for row in sample:
                print(f"   {dict(zip(columns, row))}")
        
        conn.close()
        
    except Exception as e:
        print(f"âŒ Erreur lors de la vÃ©rification: {e}")


def count_source_files():
    """
    Compte les fichiers source disponibles par type de flux.
    """
    print("ğŸ“ Analyse des fichiers source disponibles...")
    
    base_path = Path("/home/virgile/data/flux_enedis")
    
    if not base_path.exists():
        print(f"âŒ RÃ©pertoire source non trouvÃ©: {base_path}")
        return
    
    for flux_type, config in FLUX_CONFIG.items():
        if 'file_regex' in config:
            # Utiliser le regex pour compter
            pattern = config['file_regex'].replace('\\d+', '*').replace('\\.', '.')
            files = list(base_path.glob(f"**/{pattern}"))
        else:
            # Pattern par dÃ©faut
            files = list(base_path.glob(f"{flux_type}/**/*.xml"))
        
        print(f"   ğŸ“„ {flux_type}: {len(files)} fichiers trouvÃ©s")


if __name__ == "__main__":
    print("ğŸ”§ Pipeline ETL unifiÃ© pour les flux Enedis")
    print("=" * 50)
    
    # Analyser les fichiers source
    count_source_files()
    print()
    
    # ExÃ©cuter le pipeline
    success = run_pipeline()
    
    if success:
        print("\nğŸ‰ Pipeline terminÃ© avec succÃ¨s !")
    else:
        print("\nğŸ’¥ Pipeline terminÃ© avec des erreurs")